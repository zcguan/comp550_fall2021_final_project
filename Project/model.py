# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ez-Qu9vdhpW4XZLF7sY4A2iQLaUAU9I1
"""

import tensorflow as tf
import gzip
import json
import pandas as pd
import numpy as np
import datetime

# f = gzip.open('/content/drive/MyDrive/data/reviews_Movies_and_TV_5.json.gz', 'rb')
# f = gzip.open('/content/drive/MyDrive/data/cls-acl10-processed.tar.gz', 'rb')
# content = f.readlines()
# content

# d = {'reviewText':[], 'overall':[]}
# for line in content:
#   review = json.loads(line)
#   d['reviewText'].append(review['reviewText'])
#   d['overall'].append(review['overall'])
# df = pd.DataFrame.from_dict(d)
# df.to_csv('/content/drive/MyDrive/data/reviews_Movies_and_TV_5.csv', index=False)

# data_size = 20000
# df = pd.read_csv('/content/drive/MyDrive/data/reviews_Movies_and_TV_5.csv',nrows=data_size)
df = pd.read_csv('/content/drive/MyDrive/data/en.tsv', sep='\t')
# df = df.astype({'reviewText':str})

alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'"/\\|_@#$%^&*~`+-=<>()[]{}\n'
extra_char = ''
voc = alphabet + extra_char
voc_dict = {c:i+1 for i,c in enumerate(alphabet)}

input_feature_len = 1014
num_class = 2

alph_size = len(voc)
output_size = 1024
dropout = .2
num_filters = 256

df['text'] = [s[:input_feature_len].lower() for s in df['text']]
df['rating'] = [0 if r <=3 else  1 for r in df['rating']]
df.groupby('rating').count()

train_size =  int(24000 * .9)

train_inputs = df['text'][:train_size].values
train_labels = df['rating'][:train_size].values

test_inputs = df['text'][train_size:].values
test_labels = df['rating'][train_size:].values

tk = tf.keras.preprocessing.text.Tokenizer(num_words=None, char_level=True, oov_token='UNK')
tk.fit_on_texts(train_inputs)
tk.word_index = voc_dict.copy()
tk.word_index[tk.oov_token] = max(voc_dict.values()) + 1

# train
train_sequences = tk.texts_to_sequences(train_inputs)
train_data = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=input_feature_len, padding='post')
train_data = tf.keras.utils.to_categorical(train_data)

# test
test_sequences = tk.texts_to_sequences(test_inputs)
test_data = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=input_feature_len, padding='post')
test_data = tf.keras.utils.to_categorical(test_data)

train_labels = tf.keras.utils.to_categorical(train_labels)
test_labels = tf.keras.utils.to_categorical(test_labels)

model = tf.keras.models.Sequential()
# convolutional layers
# 1
model.add(tf.keras.layers.Conv1D(num_filters, kernel_size=7, padding='same', activation='relu', input_shape=(input_feature_len, len(tk.word_index)+1), kernel_initializer='random_normal'))
model.add(tf.keras.layers.MaxPooling1D(3))
# 2
model.add(tf.keras.layers.Conv1D(num_filters, kernel_size=7, padding='same', activation='relu'))
model.add(tf.keras.layers.MaxPooling1D(3))

# 3
model.add(tf.keras.layers.Conv1D(num_filters, kernel_size=3, padding='same', activation='relu'))
# 4
model.add(tf.keras.layers.Conv1D(num_filters, kernel_size=3, padding='same', activation='relu'))
# 5
model.add(tf.keras.layers.Conv1D(num_filters, kernel_size=3, padding='same', activation='relu'))
# 6
model.add(tf.keras.layers.Conv1D(num_filters, kernel_size=3, padding='same', activation='relu'))
model.add(tf.keras.layers.MaxPooling1D(3))

# fully-connected layers
model.add(tf.keras.layers.Flatten())
# 7
model.add(tf.keras.layers.Dense(output_size, activation='relu'))
model.add(tf.keras.layers.Dropout(dropout))

# 8
model.add(tf.keras.layers.Dense(output_size, activation='relu'))
model.add(tf.keras.layers.Dropout(dropout))

# 9
model.add(tf.keras.layers.Dense(2, activation='softmax'))

model.summary()

# log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

batch_size = 128
num_epochs = 10

# opt = tf.keras.optimizers.SGD(learning_rate=0.001)
opt = 'adam'

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

model.fit(train_data, train_labels, validation_split=.1, batch_size=batch_size, epochs=num_epochs)

model.save('mymodel')

pip install transformers

from transformers import CanineTokenizer, CanineForSequenceClassification
import torch

tokenizer = CanineTokenizer.from_pretrained('google/canine-s')
model = CanineForSequenceClassification.from_pretrained('google/canine-s')

# print(train_inputs.tolist)
inputs = tokenizer(train_inputs.tolist()[:100], return_tensors="pt", padding=True)
# labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
# model.train()
outputs = model(**inputs, labels=train_labels[:100])
loss = outputs.loss
logits = outputs.logits

logits



from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

from keras.layers import Input, Embedding, Activation, Flatten, Dense
from keras.layers import Conv1D, MaxPooling1D, Dropout
from keras.models import Model

train_data_source = '/content/drive/MyDrive/data/en.tsv'
test_data_source = './data/ag_news_csv/test.csv'

train_df = pd.read_csv(train_data_source, sep='\t')
test_df = train_df[20000:]
print(len(train_df.index))
print(len(test_df.index))


# convert string to lower case
train_texts = train_df['text'].values
train_texts = [s.lower() for s in train_texts]

test_texts = test_df['text'].values
test_texts = [s.lower() for s in test_texts]

# =======================Convert string to index================
# Tokenizer
tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')
tk.fit_on_texts(train_texts)
# If we already have a character list, then replace the tk.word_index
# If not, just skip below part

# -----------------------Skip part start--------------------------
# construct a new vocabulary
alphabet = "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}"
char_dict = {}
for i, char in enumerate(alphabet):
    char_dict[char] = i + 1

# Use char_dict to replace the tk.word_index
tk.word_index = char_dict.copy()
# Add 'UNK' to the vocabulary
tk.word_index[tk.oov_token] = max(char_dict.values()) + 1

# -----------------------Skip part end----------------------------

# Convert string to index
train_sequences = tk.texts_to_sequences(train_texts)
test_texts = tk.texts_to_sequences(test_texts)

# Padding
train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')
test_data = pad_sequences(test_texts, maxlen=1014, padding='post')

# Convert to numpy array
train_data = np.array(train_data, dtype='float32')
test_data = np.array(test_data, dtype='float32')

# =======================Get classes================
train_classes = train_df['rating'].values
train_class_list = [x - 1 for x in train_classes]

test_classes = test_df['rating'].values
test_class_list = [x - 1 for x in test_classes]


train_classes = to_categorical(train_class_list)
test_classes = to_categorical(test_class_list)


# =====================Char CNN=======================
# parameter
input_size = 1014
vocab_size = len(tk.word_index)
embedding_size = 69
conv_layers = [[256, 7, 3],
               [256, 7, 3],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, 3]]

fully_connected_layers = [1024, 1024]
num_of_classes = 5
dropout_p = 0.5
optimizer = 'adam'
loss = 'categorical_crossentropy'

# Embedding weights
embedding_weights = []  # (70, 69)
embedding_weights.append(np.zeros(vocab_size))  # (0, 69)

for char, i in tk.word_index.items():  # from index 1 to 69
    onehot = np.zeros(vocab_size)
    onehot[i - 1] = 1
    embedding_weights.append(onehot)

embedding_weights = np.array(embedding_weights)
print('Load')

# Embedding layer Initialization
embedding_layer = Embedding(vocab_size + 1,
                            embedding_size,
                            input_length=input_size,
                            weights=[embedding_weights])

# Model Construction
# Input
inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)
# Embedding
x = embedding_layer(inputs)
# Conv
for filter_num, filter_size, pooling_size in conv_layers:
    x = Conv1D(filter_num, filter_size)(x)
    x = Activation('relu')(x)
    if pooling_size != -1:
        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)
x = Flatten()(x)  # (None, 8704)
# Fully connected layers
for dense_size in fully_connected_layers:
    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024
    x = Dropout(dropout_p)(x)
# Output Layer
predictions = Dense(num_of_classes, activation='softmax')(x)
# Build model
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy
model.summary()

# # 1000 training samples and 100 testing samples
# indices = np.arange(train_data.shape[0])
# np.random.shuffle(indices)
#
# x_train = train_data[indices][:1000]
# y_train = train_classes[indices][:1000]
#
# x_test = test_data[:100]
# y_test = test_classes[:100]

indices = np.arange(train_data.shape[0])
np.random.shuffle(indices)

x_train = train_data[indices]
y_train = train_classes[indices]

x_test = test_data
y_test = test_classes

# Training
model.fit(x_train, y_train,
          validation_data=(x_test, y_test),
          batch_size=128,
          epochs=10,
          )